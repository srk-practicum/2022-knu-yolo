{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMAXD2Bg3fAl"
      },
      "source": [
        "### Kaggle Competition | Titanic Machine Learning from Disaster\n",
        "\n",
        ">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n",
        "\n",
        ">One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
        "\n",
        ">In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
        "\n",
        ">This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n",
        "\n",
        "From the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n",
        "\n",
        "\n",
        "### Goal for this Notebook:\n",
        "Make a simple analysis of the Titanic disaster in Python using a full complement of PyData utilities.\n",
        "\n",
        "#### Required Libraries:\n",
        "* [NumPy](http://www.numpy.org/)\n",
        "* [IPython](http://ipython.org/)\n",
        "* [Pandas](http://pandas.pydata.org/)\n",
        "* [SciKit-Learn](http://scikit-learn.org/stable/)\n",
        "* [SciPy](http://www.scipy.org/)\n",
        "* [Matplotlib](http://matplotlib.org/)\n",
        "\n",
        "***The competition's website is located on [Kaggle.com](http://www.kaggle.com/c/titanic-gettingStarted).***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQey2NMy3fAw"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhp18NQo3fAy"
      },
      "source": [
        "### Data Handling\n",
        "#### Read data (training set) in using pandas:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "J5LILRd_8NO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5eT1bfh3fA0"
      },
      "source": [
        "Show an overview of our data: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNTiYmrA3fA1"
      },
      "outputs": [],
      "source": [
        "df "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad27EQkf3fA3"
      },
      "source": [
        "### Learn more about the data:\n",
        "\n",
        "Above is a summary of data contained in a `Pandas` `DataFrame`. Think of a `DataFrame` as a Python's super charged version of the workflow in an Excel table. As you can see the summary holds quite a bit of information. First, it lets us know we have 891 observations, or passengers, to analyze here.\n",
        "\n",
        "Next it shows us all of the columns in `DataFrame`. Each column tells us something about each of our observations, like their `name`, `sex` or `age`. These colunms  are called a features of our dataset. You can think of the meaning of the words column and feature as interchangeable for this notebook. \n",
        "\n",
        "After each feature it lets us know how many values it contains. While most of our features have complete data on every observation, like the `survived` feature here: \n",
        "\n",
        "    survived    891  non-null values \n",
        "\n",
        "some are missing information, like the `age` feature: \n",
        "\n",
        "    age         714  non-null values \n",
        "\n",
        "These missing values are represented as `NaN`s.\n",
        "\n",
        "### Take care of missing values:\n",
        "The features `ticket` and `cabin` have many missing values and so can’t add much value to our analysis. To handle this we will drop them from the dataframe to preserve the integrity of our dataset. Drop waste features.\n",
        "\n",
        "Also remove NaN values from each remaining column / feature.\n",
        "\n",
        "Now we have a clean and tidy dataset that is ready for analysis. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMZbA3Aw3fA5"
      },
      "outputs": [],
      "source": [
        "# Remove ticket and cabin features\n",
        "\n",
        "# Remove NaN values\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't touch this cell\n",
        "assert \"ticket\" not in df.columns\n",
        "assert \"cabin\" not in df.columns\n",
        "assert df.isna().sum().sum() == 0"
      ],
      "metadata": {
        "id": "4i8syK1K8lZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLG-xK5t3fA6"
      },
      "source": [
        "For a detailed look at how to use pandas for data analysis, the best resource is Wes Mckinney's [book](http://shop.oreilly.com/product/0636920023784.do). Additional interactive tutorials that cover all of the basics can be found [here](https://bitbucket.org/hrojas/learn-pandas) (they're free).  If you still need to be convinced about the power of pandas check out this wirlwhind [look](http://wesmckinney.com/blog/?p=647) at all that pandas can do. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p5-6okC3fA7"
      },
      "source": [
        "### Let's take a Look at our data graphically:\n",
        "\n",
        "Plot 5 graphs in this section:\n",
        "\n",
        "\n",
        "1.   Plots a bar graph of those who survived vs those who did not. \n",
        "2.   Plots a Scatterplot of Survived distribution by Age\n",
        "3.   Plots a bar graph of Class distribution\n",
        "4.   Plots kernel density estimate of the passengers's age separately for all 3 classes\n",
        "5.   Plots a bar graph of passengers per boarding location\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMdpxz7J3fA8"
      },
      "outputs": [],
      "source": [
        "# specifies the parameters of our graphs\n",
        "fig = plt.figure(figsize=(18,6), dpi=1600) \n",
        "alpha=alpha_scatterplot = 0.2 \n",
        "alpha_bar_chart = 0.55\n",
        "\n",
        "# plot many diffrent shaped graphs together \n",
        "ax1 = plt.subplot2grid((2,3),(0,0))\n",
        "plt.title(\"Distribution of Survival, (1 = Survived)\") \n",
        "# plots a bar graph of those who surived vs those who did not.  \n",
        "\n",
        "\n",
        "plt.subplot2grid((2,3),(0,1))\n",
        "plt.title(\"Survival by Age,  (1 = Survived)\")\n",
        "\n",
        "\n",
        "ax3 = plt.subplot2grid((2,3),(0,2))\n",
        "plt.title(\"Class Distribution\")\n",
        "\n",
        "\n",
        "# plots a kernel density estimate of the passengers's age separately for all 3 classes\n",
        "plt.subplot2grid((2,3),(1,0), colspan=2)\n",
        "plt.title(\"Age Distribution within classes\")\n",
        "\n",
        "\n",
        "ax5 = plt.subplot2grid((2,3),(1,2))\n",
        "plt.title(\"Passengers per boarding location\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRihWNpk3fA9"
      },
      "source": [
        "### Exploratory Visualization:\n",
        "\n",
        "The point of this competition is to predict if an individual will survive based on the features in the data like:\n",
        " \n",
        " * Traveling Class (called pclass in the data)\n",
        " * Sex \n",
        " * Age\n",
        " * Fare Price\n",
        "\n",
        "Let’s see if we can gain a better understanding of who survived and died. \n",
        "\n",
        "\n",
        "First plot a bar graph of those who Survived Vs. Those who did not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QXHr82g3fA-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0wome_M3fA-"
      },
      "source": [
        "### Next tease more structure out of the data,\n",
        "### Break the previous graph down by gender\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35p8zc7s3fA_"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(18,6))\n",
        "\n",
        "# create a plot of two subsets, male and female, of the survived variable.\n",
        "# After that call value_counts() so it can be easily plotted as a bar graph. \n",
        "# you can use kind='barh' argument. It is just a horizontal bar graph\n",
        "\n",
        "# Next adjust graph to display the proportions of survival by gender\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBEcGFBI3fBA"
      },
      "source": [
        "Add your summaryabout recieved data for raw value counts and proportionally\n",
        "\n",
        "#### Try to go down even further:\n",
        "Can we capture more of the structure by using Pclass? Here try to bucket classes as lowest class or any of the high classes (classes 1 - 2). 3 is lowest class. Break it down by Gender and what Class they were traveling in.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4jXC0wM3fBA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to get a conclusions of the recieved data"
      ],
      "metadata": {
        "id": "yEZPJBNqFWfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XrSaN1KQ3fn1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjdgVG2_3fBC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model train and evaluation\n",
        "\n",
        "\n",
        "For all model at the end, additionaly generate:\n",
        "Normalized Confusion matrix as:\n",
        "```\n",
        "\t\t                 Predicted\n",
        "                     Survive\tNon-Survive\n",
        "Actual    Survive        0.7       0.3\n",
        "          Non-Survive    0.2       0.8\n",
        "```\n",
        "And metrics:\n",
        "```\n",
        "          Metrics\n",
        "Recall    0.2232\n",
        "Precision 0.0597\n",
        "Accuracy  0.2100\n",
        "F1 score  0.0398\n",
        "```"
      ],
      "metadata": {
        "id": "yuV-b-Jvt1PI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MLXksFt3fBD"
      },
      "source": [
        "## Supervised Machine Learning\n",
        "#### Logistic Regression:\n",
        "\n",
        "\n",
        "\n",
        "Task is to predict a binary outcome. That is, it wants to know whether some will die, (represented as a 0), or survive, (represented as 1). A good place to start is to calculate the probability that an individual observation, or person, is likely to be a 0 or 1. That way we would know the chance that someone survives, and could start making somewhat informed predictions. If we did, we'd get results like this:: \n",
        "\n",
        "![pred](https://raw.github.com/agconti/kaggle-titanic/master/images/calc_prob.png) \n",
        "\n",
        "(*Y axis is the probability that someone survives, X axis is the passenger’s number from 1 to 891.*)\n",
        "\n",
        "While that information is useful it doesn’t let us know whether someone ended up alive or dead. It just lets us know the chance that they will survive or die. We still need to translate these probabilities into the binary decision we’re looking for. But how? We could arbitrarily say that our survival cutoff is anyone with a probability of survival over 50%. In fact, this tactic would actually perform pretty well for our data and would allow you to make decently accurate predictions. Graphically it would look something like this:\n",
        "\n",
        "![predwline](https://raw.github.com/agconti/kaggle-titanic/master/images/calc_prob_wline.png)\n",
        "\n",
        "What are the odds that setting that cutoff at 50% works? Maybe 20% or 80% would work better. Clearly we need a more exact way to make that cutoff. What can save the day? In steps the **Logistic Regression**. \n",
        "\n",
        "A logistic regression follows the all steps we took above but mathematically calculates the cutoff, or decision boundary (as stats nerds call it), for you. This way it can figure out the best cut off to choose, perhaps 50% or 51.84%, that most accurately represents the training data.\n",
        "\n",
        "Below, write down the process of creating a Logitist regression model with `LogisticRegression`, training it on the data, and examining its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deeYakrn3fBF"
      },
      "outputs": [],
      "source": [
        "# instantiate and fit our model to the training data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqP3heYQ3fBF"
      },
      "outputs": [],
      "source": [
        "# Plot Predictions Vs Actual\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DKw49Rr3fBL"
      },
      "source": [
        "### Support Vector Machine (SVM)\n",
        "\n",
        "\n",
        "The logit model showed exactly where to draw our decision boundary or our 'survival cut off'. A linear line is okay, but can prediction be better? Perhaps a more complex decision boundary like a wave, circle, or maybe some sort of strange polygon would describe the variance observed in our sample better than a line.\n",
        "\n",
        "Below implement a SVM model based on `sklearn.svm.SVC` and examining the results after the SVM transforms an equation into three different mathematical plains. The first is linear, and is similar to our logic model. Next is an exponential, polynomial, transformation and finally a blank transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2XU5FihKRakX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwJyqyhF3fBN"
      },
      "source": [
        "### Random Forest\n",
        "\n",
        "the Random Forest technique is a form of non-parametric modeling that does away with all those equations created above, and uses raw computing power and a clever statistical observation to tease the structure out of the data. \n",
        "\n",
        "**How do they work?** A random forest algorithm randomly generates many extremely simple models to explain the variance observed in random subsections of our data. These models are like appraisal guesses. They are all awful individually. Really awful. But once they are averaged, they can be powerful predictive tools. The averaging step is the secret sauce. While the vast majority of those models were extremely poor; they were all as bad as each other on average. So when their predictions are averaged together, the bad ones average their effect on our model out to zero. The thing that remains, *if anything*, is one or a handful of those models have stumbled upon the true structure of the data.\n",
        "\n",
        "Below implement your model of random forest with `RandomForestClassifier`, generating predictions form the resulting model, and then scoring the results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ekx9kVAOTOKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional tasks\n",
        "\n",
        "*   Plot ROC curve for implemented models\n",
        "*   Implement prediction using another model, such as the k-nearest neighbors algorithm, decision tree classifier, gaussian naive bayes, neural network etc. Or maybe try to use sklearn.model_selection.GridSearchCV based on already implemented models.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NT6kO9UQsQit"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "Titanic_template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
