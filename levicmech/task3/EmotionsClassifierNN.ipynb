{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4KYf6Tq/pS/nmIa2wf2zw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/levicmech/2022-knu-yolo/blob/Victor_Levitskymech/levicmech/task3/EmotionsClassifierNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ofYFVrSO7S7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "\n",
        "train_nm = 'train'\n",
        "val_nm = 'val'\n",
        "\n",
        "data_transforms = {\n",
        "    train_nm: transforms.Compose([\n",
        "        transforms.Resize(128),\n",
        "        transforms.RandomCrop(128),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    val_nm: transforms.Compose([\n",
        "        transforms.Resize(128),\n",
        "        transforms.CenterCrop(128),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/emotions_ds/emotions_ds'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in [train_nm, val_nm]}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=256,\n",
        "                                             shuffle=True,) for x in [train_nm, val_nm]}\n",
        "\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in [train_nm, val_nm]}\n",
        "class_names = image_datasets[train_nm].classes"
      ],
      "metadata": {
        "id": "mOxtfdNh7Zkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names\n"
      ],
      "metadata": {
        "id": "mYKxybNaA9mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_sizes"
      ],
      "metadata": {
        "id": "GP5FFMreA_1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(dataloaders[train_nm]))\n",
        "image.numpy().shape\n"
      ],
      "metadata": {
        "id": "fcFG1L6wBDTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label\n"
      ],
      "metadata": {
        "id": "VFghiqmHBF3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = image.view(-1, *image.size()[2:])\n"
      ],
      "metadata": {
        "id": "IbXc0wOSBHzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "ax = plt.subplot(1, 1, 1)\n",
        "plt.imshow(image[0], cmap='binary_r')\n",
        "plt.title(class_names[label[0]])\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "fv3BlevSBLYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch import nn\n",
        "\n",
        "model = torchvision.models.resnet152(pretrained=True)\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "n_inputs = model.fc.in_features\n",
        "n_classes = len(class_names)\n",
        "\n",
        "model.fc = nn.Sequential(nn.Flatten(),\n",
        "                         nn.Linear(n_inputs, 128),\n",
        "                         nn.ReLU(),\n",
        "                         nn.Linear(128, n_classes),\n",
        "                         nn.Softmax(dim=1))\n",
        "\n",
        "model.aux_logits = False"
      ],
      "metadata": {
        "id": "sK5f0J_YBN-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "WBoWGuSpBRlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, epochs=15):\n",
        "    for epoch in range(epochs):\n",
        "        print(f'\\n{epoch+1} / {epochs}')\n",
        "        print('-'*20)\n",
        "        \n",
        "        for phase in [train_nm, val_nm]:\n",
        "            if phase == train_nm:\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "                \n",
        "            running_loss = 0.0\n",
        "            correct = 0\n",
        "            \n",
        "            for X, y in dataloaders[phase]:\n",
        "                X, y = X.cuda(), y.cuda()\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                with torch.set_grad_enabled(phase == train_nm):\n",
        "                    out = model(X)\n",
        "                    loss = criterion(out, y)\n",
        "                    _, preds = torch.max(out, 1)\n",
        "                    \n",
        "                    if phase == train_nm:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                \n",
        "                running_loss += loss.item() * X.size(0)\n",
        "                correct += torch.sum(preds == y.data)\n",
        "            \n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = correct.double() / len(dataloaders[phase].dataset\n",
        "                                               \n",
        "            print(f'{phase}: loss {epoch_loss:.4f}, acc {epoch_acc:.3f}')\n",
        "\n",
        "           \n",
        "train_model(model, dataloaders, criterion, optimizer, 5)"
      ],
      "metadata": {
        "id": "V64hsdAhJP4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = torch.tensor([]).to(device)\n",
        "labels = torch.tensor([]).to(device)\n",
        "\n",
        "for batch, (X, y) in enumerate(dataloaders[val_nm]):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    true_labels = torch.cat((true_labels, y), 0)\n",
        "    preds = model(X)\n",
        "    _, preds = torch.max(preds, dim=1)\n",
        "    labels = torch.cat((labels, preds), 0)\n",
        "    "
      ],
      "metadata": {
        "id": "wNhme-4zJcfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print('Classification report \\n')\n",
        "print(classification_report(true_labels.cpu(), labels.cpu(), target_names=class_names))\n",
        "conf_matrix = confusion_matrix(true_labels.cpu(), labels.cpu(), normalize='true')\n",
        "print('Confusion matrix \\n')\n",
        "print(conf_matrix.round(2))\n"
      ],
      "metadata": {
        "id": "ssRDeB2aJdRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "n_classes = len(class_names)\n",
        "lab_plt = [(true_labels.cpu() == float(i)) for i in range(n_classes)]\n",
        "outs_plt = [(labels.cpu() == float(i)) for i in range(n_classes)]\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(lab_plt[i], outs_plt[i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "\n",
        "plt.title('ROC curves', fontweight='bold', fontsize=10)\n",
        "for i in range(n_classes):\n",
        "    plt.plot(fpr[i], tpr[i], label = f'class {class_names[i]}, auc = {roc_auc[i]:.3f}' )\n",
        "\n",
        "plt.legend(loc = 'best')\n",
        "plt.plot([0,1], [0,1], linestyle='--', color='yellow')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3FWAgoU4Jgaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for img, label in dataloaders[val_nm]:\n",
        "    plt.figure(figsize=(16, 16))\n",
        "    count += 1\n",
        "    img = img.to(device)\n",
        "    pred = model(img)\n",
        "    pred = torch.max(pred, dim=1)\n",
        "    img = img.view(-1, *img.size()[2:])\n",
        "    for i in range(3):\n",
        "        ax = plt.subplot(1, 3, 1+i)\n",
        "        plt.imshow(img[i*3].cpu(), cmap='binary_r')\n",
        "        plt.title(class_names[label[0+i]])\n",
        "        plt.axis('off')\n",
        "    if count == 3:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "GykNiUyZJjch",
        "outputId": "5b8970d3-b1a1-403d-8c50-f95e24c3ece8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fc6e64f751e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_nm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OKVuUZAHJmET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}